{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Matrix Analysis\n",
    "\n",
    "In this notebook, we recall the basic elements of linear algebra which will be employed in the remainder of the notebooks. \n",
    "\n",
    "## Linear Transformation.\n",
    "\n",
    "**Definition**. Let $V$ and $W$ be vector spaces over $F$. We call a function $T:V \\to W$ a *linear transformation* from $V$ into $W$, if for all $x,y \\in V$ and $c \\in F$, we have:\n",
    "\n",
    "(a) Additivity is preserved.\n",
    "$\\begin{align}\n",
    "T(x+y) = T(x) + T(y)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "(b) Scalar-multiplication is preserved.\n",
    "$\\begin{align}\n",
    "T(cx) = cT(x)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "We often simply call $T$ a *linear map*. If $V = W$, then $T$ is called a *linear operator*.\n",
    "\n",
    "We turn our attention to two important sets associated with linear maps: the *range space* and the *null space*. The determination of these sets allows us to more closely examine the intrinsic properties of a linear transformation.\n",
    "\n",
    "**Definition**. Let $V$ and $W$ be vector spaces, and $T:V\\to W$ be linear. We define the *null space* (or *kernel*) $N(T)$ of $T$ to be the set of all vectors $x$ in $V$, such that $T(x) = 0$. That is,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "N(T) := \\{x : T(x) = 0 \\}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $0$ represents the zero-vector in $W$.\n",
    "\n",
    "We define the *range* (or *image*) $R(T)$ of $T$ to be the subset of $W$ consisting of all images (under $T$) of vectors in $V$; that is $R(T) = \\{T(x):x \\in V\\}$.\n",
    "\n",
    "**Theorem** 1. Let $V$ and $W$ be vector spaces and $T:V\\to W$ be linear. Then, $N(T)$ and $R(T)$ are subspaces of $V$ and $W$ respectively.\n",
    "\n",
    "**Proof.**\n",
    "\n",
    "Define $0_V$ and $0_W$ to be the zero vectors in $V,W$ respectively.\n",
    "\n",
    "Claim. $N(T) \\subseteq V$\n",
    "\n",
    "(1) Since $T(0_V) = 0_W$, we have that $0_V \\in N(T)$. So,the zero vector belongs to $N(T)$.\n",
    "\n",
    "(2) Let $x,y \\in N(T)$. Then, $T(x+y) = T(x) + T(y) = 0_W + 0_W = 0_W$. Hence, $x+y \\in N(T)$. Thus, $N(T)$ is closed under vector addition.\n",
    "\n",
    "(3) Let $c \\in F$. Then, $T(cx) = cT(x) = c \\cdot 0_W = 0_W$. So, $cx \\in N(T)$. Thus, $N(T)$ is closed under scalar multiplication. \n",
    "\n",
    "Consequently, $N(T)$ is a subspace of $V$.\n",
    "\n",
    "Claim. $R(T) \\subseteq W$\n",
    "\n",
    "(1) Since $T(0_V) = 0_W$, the zero vector $0_W$ belongs to $R(T)$.\n",
    "\n",
    "(2) Let $x,y \\in R(T)$. Then, by the very definition of a range space, there exist $v,w \\in V$, such that $T(v) = x, T(w) = y$. Since, $V$ is a vector space, $v + w \\in V$. So, $T(v + w) = T(v) + T(w) = x + y$. It follows that, $x + y \\in R(T)$. $R(T)$ is closed under vector addition.\n",
    "\n",
    "(3) On similar lines, let $x \\in R(T), c \\in F$. Then, by the very definition of a range space, there exists $v \\in V$, such that $T(v) = x$. $V$ is a vector space, so $cv \\in V$. Therefore, $T(cv) = cT(v) = cx$. It follows that, $cx \\in R(T)$. $R(T)$ is closed under scalar multiplication.\n",
    "\n",
    "Consequently, $R(T)$ is a subspace of $W$.\n",
    "\n",
    "**Theorem** 2. Let $V$ and $W$ be vector spaces, and let $T:V \\to W$ be a linear map. If $B = \\{ v_1,v_2,\\ldots,v_n \\}$ is a basis for $V$, then \n",
    "\n",
    "$\\begin{align}\n",
    "R(T) = span(T(B)) = span(\\{Tv_1,Tv_2,\\ldots,Tv_n\\})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Proof**.\n",
    "\n",
    "Clearly, $T(v_i) \\in R(T)$ for each $i$. Because, $R(T)$ is a subspace, $R(T)$ contains all linear combinations of $Tv_1,Tv_2,\\ldots,Tv_n$. So, $R(T)$ contains $span(\\{Tv_1,Tv_2,\\ldots,Tv_n\\})$. But, we know, that the span of any subset $S$ of a vector space $V$, is a subspace of $V$. Therefore, in the $\\Rightarrow$ direction, $span(\\{Tv_1,Tv_2,\\ldots,Tv_n\\}) \\subseteq V$.\n",
    "\n",
    "In the opposite direction $\\Leftarrow$, suppose that $w \\in R(T)$. Then, $w = T(v)$ for some $v \\in V$ by the definition of a range space. Because, $B$ is a basis for $V$, we have:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "v = \\sum_{i=1}^{n} \\alpha_i v_i\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Since, $T$ is linear, it follows that \n",
    "\n",
    "$\\begin{align}\n",
    "w = T(v) = \\sum_{i=1}^{n} \\alpha_i T(v_i) \\in span(T(B))\n",
    "\\end{align}$\n",
    "\n",
    "So, $R(T) \\subseteq span(T(B))$. \n",
    "\n",
    "Consequently, $R(T)= span(T(B))$.\n",
    "\n",
    "**Definition**. Let $V$ and $W$ be vector spaces and let $T:V \\to W$ be linear. If $N(T)$ and $R(T)$ are finite dimensional, then we define the *nullity* of $T$, denoted $nullity(T)$ and the *rank* of $T$, denoted $rank(T)$, to be the dimensions of $N(T)$ and $R(T)$ respectively.\n",
    "\n",
    "Reflecting on the action of a linear transformation, we see intuitively that the larger the nullity, the smaller the rank. In other words, the more vectors that are carried into $0$, the smaller the range. The same heuristic reasoning tells us that the larger the rank, the smaller the nullity. This balance between rank and nullity is made precise in the next theorem, appropriately called the rank-nullity-dimension theorem.\n",
    "\n",
    "**Theorem** 3 (*Rank Nullity Dimension Theorem*). Let $V$ and $W$ be vector spaces, and let $T:V\\to W$ be linear. If $V$ is finite dimensional, then\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "nullity(T) + rank(T) = dim(V)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "**Proof**.\n",
    "\n",
    "Suppose that $dim(V) = n$, $dim(N(T)) = k$ and $\\{v_1,v_2,\\ldots,v_k\\}$ is a basis for $N(T)$. Recall from basic linear algebra, that we may extend the basis $\\{v_1,v_2,\\ldots,v_k\\}$ to a basis $B = \\{v_1,v_2,\\ldots,v_n\\}$ for $V$, as long as the new vectors added are not in the span of the previous ones. We claim that $S = \\{T(v_{k+1},T(v_{k+2}),\\ldots,T(v_n)\\}$ is a basis for $R(T)$. \n",
    "\n",
    "(1) First we prove that $S$ generates $R(T)$.\n",
    "\n",
    "Using theorem (2) and the fact that $T(v_i) = 0$ for $1 \\le i \\le k$ we have:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "R(T) &= span(\\{Tv_1,Tv_2,\\ldots,Tv_n\\}\\\\\n",
    "&= span(\\{T(v_{k+1}),T(v_{k+2},\\ldots, T(v_{n}))\\}) = span(S)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "(2) Now we prove that $S$ is linearly independent. Suppose that,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=k+1}^{n} \\beta_i T(v_i) = 0\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Using the fact that $T$ is linear, we have:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "T\\left(\\sum_{i=k+1}^{n} \\beta_i v_i\\right) = 0\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "So,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=k+1}^{n} \\beta_i v_i \\in N(T)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Hence, there exists $c_1,c_2,\\ldots,c_k \\in F$ such that \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=k+1}^{n} \\beta_i v_i = \\sum_{i=1}^{n} c_i v_i\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "or \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{n} (-c_i) v_i + \\sum_{i=k+1}^{n} \\beta_i v_i = 0\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Since, $B$ is a basis for $V$, the vectors $v_i$ are linearly independent. So, all of the coefficients $\\beta_i$'s must be equal to $0$. This implies that $S$ is linearly independent.\n",
    "\n",
    "Consequently, $S$ is a basis for $R(T)$ and $rank(T) = n - k$. Hence, $rank(T) + nullity(T) = dim(V)$.\n",
    "\n",
    "## Matrices.\n",
    "\n",
    "Let $m$ and $n$ be two positive integers. We call a *matrix* having *m* rows and *n* columns, or a matrix $m \\times n$, or a matrix $(m,n)$ with elements in $K$, a set of $mn$ scalars $a_{ij} \\in K$, with $i=1,\\ldots,m$ and $j=1,\\ldots,n$ represented by the following rectangular array\n",
    "\n",
    "$A = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\ldots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\ldots & a_{2n}\\\\\n",
    "\\vdots & \\vdots &        & \\vdots\\\\\n",
    "a_{m1} & a_{m2} & \\ldots & a_{mn} \\tag{1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "When $K = \\mathbb{R}$ or $\\mathbb{C}$, I shall write $A \\in \\mathbb{R}^{m\\times n}$ or $A \\in \\mathbb{C}^{m\\times n}$, to explicitly outline the numerical fields to which the elements of $A$ belong to.\n",
    "\n",
    "From basic linear algebra, it is worthwhile to keep in mind that, *matrices are concrete realizations of linear transformations from $\\mathbb{R}^n$ to $\\mathbb{R}^m$*. $A$ is a map $T:\\mathbb{R}^n \\to \\mathbb{R}^m$, where $T \\in L(V,W)$, the space of all linear transformations.\n",
    "\n",
    "In particular, let $V$, $W$ be finite dimensional vector spaces. Let $T$ be a linear transformation from $v$ into $W$.\n",
    "\n",
    "$$T:V \\to W$$\n",
    "\n",
    "Let $B_v = \\{v_1,v_2,\\ldots,v_n\\}$ and $B_w = \\{w_1,w_2,\\ldots,w_m\\}$ be ordered bases of the vector spaces $V$ and $W$ respectively. $dim(V)= n$ and $dim(W) = m$. The matrix of the linear transformation $T$ is defined as follows.\n",
    "\n",
    "A linear transformation is completely determined by its action on the basis vectors. If we know $Tv_1,Tv_2,\\ldots,Tv_n$, it is enough to completely determine $T$. \n",
    "\n",
    "Each of the vectors $Tv_1,Tv_2,\\ldots,Tv_n$ are elements of the vector space $W$, so we can resolve them in terms of the basis vectors $w_1,w_2,\\ldots,w_m$. \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Tv_1 &= a_{11}w_1 + a_{21}w_2 + \\ldots + a_{m1}w_m \\\\\n",
    "Tv_2 &= a_{12}w_1 + a_{22}w_2 + \\ldots + a_{m2}w_m \\\\\n",
    "\\vdots\\\\\n",
    "Tv_n &= a_{1n}w_1 + a_{2n}w_2 + \\ldots + a_{mn}w_m \n",
    "\\end{align} \\tag{2}\n",
    "$\n",
    "\n",
    "That is, \n",
    "$\n",
    "\\begin{align}\n",
    "Tv_j = \\sum_{i=1}^{m}a_{ij} w_i\n",
    "\\end{align} \\tag{3}\n",
    "$\n",
    "\n",
    "On the right hand side, $i$ is the running index, $j$ is the free index that corresponds to $Tv_j$. The matrix of the vector $Tv_j$ relative to the basis $B_w$ is the column vector whose entries are the coordingates with respect to $B_w$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "[Tv_j]_{B_W} = \n",
    "\\begin{bmatrix}\n",
    "a_{1j}\\\\\n",
    "a_{2j}\\\\\n",
    "\\vdots\\\\\n",
    "a_{mj}\n",
    "\\end{bmatrix}\n",
    "\\end{align} \\tag{4}\n",
    "$\n",
    "\n",
    "The matrix of the linear transformation $T$, that sends $x \\in V$ having coordingates $\\mathbf{x} = (x_1,x_2,\\ldots,x_n)$ with respect to $B_W$ is defined as:\n",
    "\n",
    "$\\displaystyle  \\begin{array}{{>{\\displaystyle}l}}\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\begin{matrix}\n",
    "Tv_{1} & Tv_{2}\n",
    "\\end{matrix} \\ \\ \\dotsc \\ \\ \\ Tv_{n} \\ \\\\\n",
    "A\\ =\\ ( a_{ij}) =[ T]^{B_{W}}_{B_{V}} =\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\  & \\dotsc  & a_{1n}\\\\\n",
    "a_{21} & a_{22} \\  & \\dotsc  & a_{2n}\\\\\n",
    "a_{31} & a_{32} \\  & \\dotsc  & a_{3n}\\\\\n",
    "\\vdots  &  &  & \\\\\n",
    "a_{m1} & a_{m2} \\  & \\dotsc  & a_{mn}\n",
    "\\end{bmatrix}\\begin{matrix}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\\\\\n",
    "\\vdots \\\\\n",
    "w_{m}\n",
    "\\end{matrix}\n",
    "\\end{array} \\tag{5}$\n",
    "\n",
    "As an aid to remembering, how $[T]_{B_V}^{B_W}$ is constructed from $T$, you might write the vectors $Tv_1,Tv_2,\\ldots,Tv_n$ across the top and the basis vectors $w_1,w_2,\\ldots,w_m$ for the target space along the right. In the matrix above, the $j$th column of $[T]_{B_V}^{B_W}$ consists of scalars needed to write $Tv_j$ as a linear combination of the $w$'s. Thus, the picture should remind you that $Tv_j$ is retrieved by multiplying each entry in the $j$th column, by the corresponding $w$ from the right and then adding up the resulting vectors. This is in conformation with the usual notion of writing a matrix.\n",
    "\n",
    "## Row space and column space of a matrix.\n",
    "\n",
    "Suppose, we are to solve a system of linear equations\n",
    "$\n",
    "\\begin{align}\n",
    "Ax = b\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Writing $A = [A_1 A_2 \\ldots A_n]$, we have,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "[A_1 A_2 \\ldots A_n]\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{bmatrix} &= b \\\\\n",
    "x_1 A_1 + x_2 A_2 + \\ldots + x_n A_n &= b\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Suppose $A$ is not invertible, then $Ax=b$ is solvable for some right hand side vectors $b$, and not solvable for other right hand vectors. We want to describe the good right hand side vectors $b$ - the vectors that can be written as a linear combination of the column vectors of $A$. Those $b$'s form the column space $A$. \n",
    "\n",
    "**Definition**. The *column space* of $A$ is the subspace generated by all linear combinations of the columns of the matrix $A$.\n",
    "\n",
    "Remember that, $Ax = b$ is solvable, if and only, if $b$ is in the column space of $A$. Since, $b \\in \\mathbb{R}^m$, the column space of $A$ is a subspace of $\\mathbb{R}^m$.\n",
    "\n",
    "**Definition**. Let $A$ be a matrix of order $m \\times n$ over the field of real numbers $\\mathbb{R}$. $A \\in \\mathbb{R}^{m \\times n}$. The subspace of $\\mathbb{R}^n$ generated by the row-vectors of $A$ is called the row-space of $A$. \n",
    "\n",
    "The dimension of the row space of $A$ is called the *row rank* of $A$ The dimension of the column space of $A$ is called the *column rank* of $A$. \n",
    "\n",
    "\n",
    "## Elementary matrix operations and elementary matrices.\n",
    "\n",
    "Solving a system of linear algebraic equations $Ax=b$ is the most important aspect of linear algebra. From high-school and university, we are familiar with performing elementary row-operations on a matrix $A$, resulting in a simplified system of equations easier to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
